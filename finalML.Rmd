---
title: "Final ML Project"
author: "Lulu T"
date: "26/05/2021"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE, fig.align='center', fig.width=2, fig.height=2, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
library(AppliedPredictiveModeling)
library(lattice)
library(caret)
library(readr)
library(rattle)
library(randomForest)
library(rpart)
library(e1071)
```


## Download the data 
Download the data and change the outcome variable to a factor variable. 

```{r }
traindata <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings = c(" ","","#DIV/0!","NA"))
testdata <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings = c(" ","","#DIV/0!", "NA"))
traindata$classe <- as.factor(traindata$classe)
dim(traindata)
```

## Clean up the data:

Get rid of variables that have more that 20% NA values. 
```{r}
maxNA <- 0.2 * nrow(traindata)
NAcolumns <- which(colSums(is.na(traindata)) > maxNA)
trainfinal <- traindata[,-NAcolumns]
testfinal <- testdata[,-NAcolumns]
dim(trainfinal)
names(trainfinal)
```
Can further narrow data set by eliminating variables that have near zero variability or are useless in predicting classe such as time stamp, new_window and user name variables.
```{r}
removetrain <- grep("timestamp|name|new_window|X", names(trainfinal))
removetest <- grep("timestamp|name|new_window|X|problem_id", names(testfinal))
trainfinal <- trainfinal[,-c(removetrain)]
testfinal <-  testfinal[, -c(removetest)]
dim(trainfinal)
dim(testfinal)

```
## Cross validation - Create data partitions, 
- Cross validation was performed by by partitioning our training data set randomly without replacement into 2 sub samples, one for the training set (0.75) and the other for the testing set (0.25). They were partitioned using createDataPartition function.  Then used Boostrap resampling in the training data set for cross validation. 

```{r }
inTrain = createDataPartition(trainfinal$classe, p = 3/4)[[1]]
training = trainfinal[ inTrain,]  
testing = trainfinal[-inTrain,] 
plot(training$classe, main="Frequency of each Classe in Training Set", xlab="classe", ylab="Frequency")
```

## Variables to include in models
- Tested which variable correlate highly to select the best variables to include in model thereby reducing the noise of the model. Based on the results, we decided to include all the variables in the model. 
```{r}
corvar <- training[, -54]
M <- abs(cor(corvar))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
```


## Create models:
- We created several models using all the variables from the training data set, to predict classe in the test data set. The accuracy of each model was measured and then each model was used to predict the final test set for this assignment. However the model with the highest accuracy was used in the quiz. The models used are the following: Random forest, SVM and decision trees.These three models are best for classification predictions. 

## The expected out-of-sample error:
- The expected out-of-sample error will correspond to the "accuracy" measure in the cross-validation data of the model used. This will be obtained from the confusionMatrix function that compares the prediction to actual classe in testing data set. "Accuracy" is the proportion of correct classified observation over the total count of sample in the testing data set. We expect the out-of-sample error to be similar to initial testing accuracy measure because we avoided over fitting in our models i.e. reduced bias. 

# First model: Random Forest 

```{r}
train_control <- trainControl(method="boot", number=100)
mod1 <- randomForest(classe~., data=training, trControl = train_control)
plot(mod1)
pred1 <- predict(mod1,testing)
accuracyrf <- confusionMatrix(pred1, testing$classe)$overall[1]
accuracyrf
```
- The out of sample error is 1 - Accuracy rate. Which is 0.0018

# Second Model: Decision Tree

```{r}
mod2 <- train(classe ~ ., method = "rpart", data=training, trControl = train_control)
pred2 <-  predict(mod2, testing)
accuracyrpart <- confusionMatrix(pred2, testing$classe)$overall[1]
accuracyrpart
```


# Third Model: SVM
```{r}
mod3 <-  svm(classe ~. , data=training, trControl = train_control )
pred3 <- predict(mod3,testing)
accuracysvm <- confusionMatrix(pred3, testing$classe)$overall[1]
```

The accuracy ratings are as follows:
 - for random forest : `r accuracyrf`  
 - for decision tree : `r accuracyrpart`  
 - for svm:  `r accuracysvm`  

## Predicting the test data set with all three models

```{r }
predictfinal1 <- predict(mod1, newdata =testfinal)
predictfinal1
```

```{r }
predictfinal2 <- predict(mod2, newdata = testfinal)
predictfinal2
```

```{r }
predictfinal3 <- predict(mod3, newdata = testfinal)
predictfinal3
```

-The first model(Random Forest) predictions were submitted to the quiz, and they showed an accuracy of 100%. 



